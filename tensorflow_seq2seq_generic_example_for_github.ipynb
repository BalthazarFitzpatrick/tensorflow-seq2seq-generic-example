{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# math stuff\n",
    "from math import floor\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# TensorFlow\n",
    "import tensorflow as tf\n",
    "\n",
    "# suppress numpy scientific e notation\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflow version\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dummy Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dummy data similar to what my database query returns (several features over time)\n",
    "sequence = np.float64(np.arange(20000).reshape((5000, 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Parameters and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence variables\n",
    "observation_length = 240 # will cause encoder lstm cells to unroll 'observation_length' times\n",
    "prediction_length = 12 # will cause decoder lstm cells to unroll 'prediction_length' times\n",
    "stride = 2 # window for batch generation will slide by 'stride' time steps\n",
    "combined_length = observation_length + prediction_length\n",
    "total_sequence_length = len(sequence)\n",
    "train_sequence_length = floor(total_sequence_length * 0.6)\n",
    "val_sequence_length = floor(total_sequence_length * 0.4)\n",
    "\n",
    "# compute number of batches to emit\n",
    "num_of_train_seq_batches = floor((train_sequence_length - combined_length) / stride)\n",
    "num_of_val_seq_batches = floor((val_sequence_length - combined_length) / stride)\n",
    "print(\"# of batches in training sequence:\", num_of_train_seq_batches)\n",
    "print(\"# of batches in validation sequence:\", num_of_val_seq_batches)\n",
    "\n",
    "# number of features going into the encoder\n",
    "features_enc_inp = len(sequence[1])\n",
    "\n",
    "# number of features of the target sequence\n",
    "features_dec_inp = 1\n",
    "features_dec_exp_out = features_dec_inp\n",
    "\n",
    "# number of batches used in each iteration\n",
    "batch_size = 3\n",
    "print(\"batch size:\", batch_size)\n",
    "\n",
    "# defining layers and number of units for basic lstm cells\n",
    "enc_num_cells = 2 # how many lstm cells are we using\n",
    "enc_num_units = 240 # how many lstm units, or commonly known as hidden dimensions/neurons, shall each lstm cell have\n",
    "dec_num_cells = enc_num_cells\n",
    "dec_num_units = enc_num_units\n",
    "\n",
    "# optimizer variables\n",
    "learning_rate = 0.0003\n",
    "lr_decay = 0.95\n",
    "momentum = 0.5\n",
    "lambda_l2_reg = 0.02\n",
    "\n",
    "# dropout\n",
    "dropout_keep_prob = 0.5\n",
    "\n",
    "# training parameters\n",
    "num_batches_train = floor(num_of_train_seq_batches / batch_size)\n",
    "num_batches_val = floor(num_of_val_seq_batches / batch_size)\n",
    "num_epochs_train = 100\n",
    "early_stop_limit = 100\n",
    "print(\"# of training steps:\", num_batches_train)\n",
    "print(\"# of validation steps:\", num_batches_val)\n",
    "train_to_val_ratio = floor(num_batches_train / num_batches_val)\n",
    "print(\"run validation op every:\", train_to_val_ratio, \"training steps\")\n",
    "num_batches_train = train_to_val_ratio * num_batches_val\n",
    "print(\"new # of batches for training sequence:\", num_batches_train)\n",
    "print(\"early stop after:\", early_stop_limit, \"validation iterations without improvement\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Generator for Training Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define generator function for training data\n",
    "def gen_train():\n",
    "    train_sequence = sequence[0:train_sequence_length]\n",
    "    \n",
    "    # transform and emit data in batches\n",
    "    for i in range(0, num_of_train_seq_batches * stride, stride):\n",
    "        result = np.array(train_sequence[i:i + combined_length])\n",
    "        scaler = MinMaxScaler()\n",
    "        result = scaler.fit_transform(result, y=None)\n",
    "        \n",
    "        # flip array upside down as data is ordered by date desc\n",
    "        result_flipped = np.flipud(result)\n",
    "        \n",
    "        # yield results\n",
    "        yield result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Generator for Validation Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define generator function for validation data\n",
    "def gen_val():\n",
    "    val_sequence = sequence[train_sequence_length:train_sequence_length+val_sequence_length]\n",
    "    \n",
    "    # transform and emit data in batches\n",
    "    for i in range(0, num_of_val_seq_batches * stride, stride):\n",
    "        result = np.array(val_sequence[i:i + combined_length])\n",
    "        scaler = MinMaxScaler()\n",
    "        result = scaler.fit_transform(result, y=None)\n",
    "        \n",
    "        # flip array upside down as data is ordered by date desc\n",
    "        result_flipped = np.flipud(result)\n",
    "        \n",
    "        # yield results\n",
    "        yield result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reset Default TensorFlow Graph and Create Sequence Length Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset default tf graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# length of input and output\n",
    "seq_length_inp = tf.fill([batch_size], observation_length)\n",
    "seq_length_out = tf.fill([batch_size], prediction_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create TensorFlow Training Input and Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create placeholder for training next element\n",
    "train_next_element = tf.placeholder(\n",
    "    tf.float32, \n",
    "    shape=(combined_length, features_enc_inp), \n",
    "    name=\"next_train_element_from_generator\")\n",
    "\n",
    "# create training dataset from generator\n",
    "train_dataset = tf.data.Dataset.from_generator(\n",
    "    gen_train,\n",
    "    (tf.float32),\n",
    "    (tf.TensorShape([combined_length, features_enc_inp]))\n",
    "    )\n",
    "\n",
    "# prefetch 'number of batches' training sequences of data\n",
    "train_prefetched = train_dataset.prefetch(num_of_train_seq_batches)\n",
    "\n",
    "# shuffle training batches\n",
    "train_buffer_size = tf.constant(\n",
    "    num_of_train_seq_batches,\n",
    "    dtype=tf.int64)\n",
    "\n",
    "train_shuffled = train_prefetched.shuffle(\n",
    "    train_buffer_size,\n",
    "    seed=None\n",
    ")\n",
    "\n",
    "# batch training batches together 'batch size' times\n",
    "train_batched = train_shuffled.batch(batch_size)\n",
    "\n",
    "# create initializable training iterator\n",
    "train_iterator = train_batched.make_initializable_iterator()\n",
    "train_next_element = train_iterator.get_next()\n",
    "\n",
    "# create training encoder input slice \n",
    "# from [first batch, first time step in observation sequence, first feature]\n",
    "# to [last batch, last time step in observation sequence, last feature]\n",
    "train_enc_inp = tf.slice(\n",
    "    train_next_element,\n",
    "    [0, 0, 0],\n",
    "    [batch_size, observation_length, features_enc_inp])\n",
    "\n",
    "# create training decoder input slice \n",
    "# from [first batch, first time step in prediction sequence, first feature]\n",
    "# to [last batch, last time step in prediction sequence, first feature]\n",
    "train_dec_inp = tf.slice(\n",
    "    train_next_element,\n",
    "    [0, observation_length, 0],\n",
    "    [batch_size, prediction_length, features_dec_inp])\n",
    "\n",
    "# create training decoder expected output slice \n",
    "# from [first batch, first time step in prediction sequence, first feature]\n",
    "# to [last batch, last time step in prediction sequence, first feature]\n",
    "train_dec_exp_out = tf.slice(\n",
    "    train_next_element,\n",
    "    [0, observation_length, 0],\n",
    "    [batch_size, prediction_length, features_dec_exp_out])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create TensorFlow Validation Input and Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create placeholder for validation next element\n",
    "val_next_element = tf.placeholder(\n",
    "    tf.float32, \n",
    "    shape=(combined_length, features_enc_inp), \n",
    "    name=\"next_val_element_from_generator\")\n",
    "\n",
    "# create validation dataset from generator\n",
    "val_dataset = tf.data.Dataset.from_generator(\n",
    "    gen_val,\n",
    "    (tf.float32),\n",
    "    (tf.TensorShape([combined_length, features_enc_inp]))\n",
    "    )\n",
    "\n",
    "# prefetch 'number of batches' validation sequences of data\n",
    "val_prefetched = val_dataset.prefetch(num_of_val_seq_batches)\n",
    "\n",
    "# shuffle validation batches\n",
    "val_buffer_size = tf.constant(\n",
    "    num_of_val_seq_batches,\n",
    "    dtype=tf.int64)\n",
    "\n",
    "val_shuffled = val_prefetched.shuffle(\n",
    "    val_buffer_size,\n",
    "    seed=None\n",
    ")\n",
    "\n",
    "# batch validation batches together 'batch size' times\n",
    "val_batched = val_shuffled.batch(batch_size)\n",
    "\n",
    "# create initializable validation iterator\n",
    "val_iterator = val_batched.make_initializable_iterator()\n",
    "val_next_element = val_iterator.get_next()\n",
    "\n",
    "# create validation encoder input slice \n",
    "# from [first batch, first time step in observation sequence, first feature]\n",
    "# to [last batch, last time step in observation sequence, last feature]\n",
    "val_enc_inp = tf.slice(\n",
    "    val_next_element,\n",
    "    [0, 0, 0],\n",
    "    [batch_size, observation_length, features_enc_inp])\n",
    "\n",
    "# create validation decoder input slice \n",
    "# from [first batch, first time step in prediction sequence, first feature]\n",
    "# to [last batch, last time step in prediction sequence, first feature]\n",
    "val_dec_inp = tf.slice(\n",
    "    val_next_element,\n",
    "    [0, observation_length, 0],\n",
    "    [batch_size, prediction_length, features_dec_inp])\n",
    "\n",
    "# create validation decoder expected output slice \n",
    "# from [first batch, first time step in prediction sequence, first feature]\n",
    "# to [last batch, last time step in prediction sequence, first feature]\n",
    "val_dec_exp_out = tf.slice(\n",
    "    val_next_element,\n",
    "    [0, observation_length, 0],\n",
    "    [batch_size, prediction_length, features_dec_exp_out])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Encoder and Decoder Cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('Seq2seq', reuse = tf.AUTO_REUSE):\n",
    "\n",
    "    # create encoder cells\n",
    "    enc_cells = []\n",
    "    for i in range(enc_num_cells):\n",
    "        enc_cells.append(tf.nn.rnn_cell.BasicLSTMCell(enc_num_units))\n",
    "    enc_cell = tf.nn.rnn_cell.MultiRNNCell(enc_cells)\n",
    "    enc_cell = tf.nn.rnn_cell.DropoutWrapper(enc_cell, output_keep_prob = dropout_keep_prob)\n",
    "\n",
    "    # create decoder cells\n",
    "    dec_cells = []\n",
    "    for i in range(dec_num_cells):\n",
    "        dec_cells.append(tf.nn.rnn_cell.BasicLSTMCell(dec_num_units))\n",
    "    dec_cell = tf.nn.rnn_cell.MultiRNNCell(dec_cells)\n",
    "    dec_cell = tf.nn.rnn_cell.DropoutWrapper(dec_cell, output_keep_prob = dropout_keep_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('Seq2seq', reuse = tf.AUTO_REUSE): \n",
    "\n",
    "    # define training encoder\n",
    "    train_enc_out, train_enc_state = tf.nn.dynamic_rnn(\n",
    "        enc_cell, \n",
    "        train_enc_inp,\n",
    "        dtype = tf.float32,\n",
    "        sequence_length = seq_length_inp,\n",
    "        time_major = time_major)\n",
    "\n",
    "    # define training training helper\n",
    "    train_training_helper = tf.contrib.seq2seq.TrainingHelper(\n",
    "        train_dec_inp, \n",
    "        seq_length_out, \n",
    "        time_major = time_major)\n",
    "\n",
    "    # define training decoder\n",
    "    train_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "        dec_cell, \n",
    "        train_training_helper, \n",
    "        train_enc_state)\n",
    "\n",
    "    # define training dynamic decoding\n",
    "    train_dec_out, train_dec_state, train_dec_out_seq_length = tf.contrib.seq2seq.dynamic_decode(\n",
    "        train_decoder, \n",
    "        output_time_major = time_major)\n",
    "\n",
    "    # extract training logits from decoder output\n",
    "    train_dec_out_logits = train_dec_out.rnn_output\n",
    "\n",
    "    # training dense layer to reduce output to 'features_dec_exp_out' feature(s)\n",
    "    train_output_dense = tf.layers.dense(\n",
    "        train_dec_out_logits,\n",
    "        features_dec_exp_out)\n",
    "\n",
    "    # training loss function\n",
    "    train_loss = tf.reduce_mean(tf.nn.l2_loss(train_output_dense - train_dec_exp_out))\n",
    "\n",
    "    # l2 regularization using all variables except for bias and batch_resample\n",
    "    l2 = lambda_l2_reg * sum(\n",
    "        tf.nn.l2_loss(tf_var)\n",
    "            for tf_var in tf.trainable_variables()\n",
    "            if not (\"noreg\" in tf_var.name or \"Bias\" in tf_var.name)\n",
    "    )\n",
    "    train_loss += l2\n",
    "\n",
    "    # optimizer type and variables\n",
    "    optimizer = tf.train.RMSPropOptimizer(\n",
    "        learning_rate,\n",
    "        decay = lr_decay,\n",
    "        momentum = momentum,\n",
    "        epsilon = 1e-10)\n",
    "\n",
    "    # train operation\n",
    "    train_op = optimizer.minimize(train_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Validation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('Seq2seq', reuse = tf.AUTO_REUSE):    \n",
    "    \n",
    "    # define validation encoder\n",
    "    val_enc_out, val_enc_state = tf.nn.dynamic_rnn(\n",
    "        enc_cell, \n",
    "        val_enc_inp,\n",
    "        dtype = tf.float32,\n",
    "        sequence_length = seq_length_inp,\n",
    "        time_major = time_major)\n",
    "    \n",
    "    # define validation training helper\n",
    "    val_training_helper = tf.contrib.seq2seq.TrainingHelper(\n",
    "        val_dec_inp, \n",
    "        seq_length_out, \n",
    "        time_major = time_major)\n",
    "    \n",
    "     # define validation decoder\n",
    "    val_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "        dec_cell, \n",
    "        val_training_helper, \n",
    "        val_enc_state)\n",
    "    \n",
    "    # define validation dynamic decoding\n",
    "    val_dec_out, val_dec_state, val_dec_out_seq_length = tf.contrib.seq2seq.dynamic_decode(\n",
    "        val_decoder, \n",
    "        output_time_major = time_major)\n",
    "    \n",
    "    # extract validation logits from decoder output\n",
    "    val_dec_out_logits = val_dec_out.rnn_output\n",
    "    \n",
    "    # validation dense layer to reduce output to 'features_dec_exp_out' feature(s)\n",
    "    val_output_dense = tf.layers.dense(\n",
    "        val_dec_out_logits,\n",
    "        features_dec_exp_out)\n",
    "    \n",
    "    # validation loss function\n",
    "    val_loss = tf.reduce_mean(tf.nn.l2_loss(val_output_dense - val_dec_exp_out))\n",
    "    \n",
    "    # l2 regularization using all variables except for bias and batch_resample\n",
    "    l2 = lambda_l2_reg * sum(\n",
    "        tf.nn.l2_loss(tf_var)\n",
    "            for tf_var in tf.trainable_variables()\n",
    "            if not (\"noreg\" in tf_var.name or \"Bias\" in tf_var.name)\n",
    "    )\n",
    "    val_loss += l2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Trainable Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tf_var in tf.trainable_variables():\n",
    "    print(tf_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Training and Validation Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# define session\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# initialize global variables / check for uninitialized variables\n",
    "sess.run([tf.global_variables_initializer(), tf.local_variables_initializer()])\n",
    "\n",
    "# check for unitialized variables \n",
    "# print(sess.run(tf.report_uninitialized_variables())\n",
    "\n",
    "lowest_val_loss = 100.0\n",
    "early_stop_counter = 0\n",
    "\n",
    "# run training and validation\n",
    "print(\"epoch batch training_loss validation_loss lowest_val_loss early_stop_counter\")\n",
    "for epoch in range(num_epochs_train):\n",
    "    sess.run([train_iterator.initializer, val_iterator.initializer])\n",
    "    for batch in range(num_batches_train):\n",
    "        if early_stop_counter <= early_stop_limit:\n",
    "            try:\n",
    "                train_results = sess.run([train_op, train_loss])\n",
    "                if batch % train_to_val_ratio == 0:\n",
    "                    try:\n",
    "                        val_results = sess.run(val_loss)\n",
    "                    except tf.errors.OutOfRangeError:\n",
    "                        print(\"end of validation dataset\")\n",
    "                    if val_results < lowest_val_loss:\n",
    "                        lowest_val_loss = val_results\n",
    "                        early_stop_counter = 0\n",
    "                    else:\n",
    "                        early_stop_counter += 1\n",
    "                    print(epoch, batch, train_results[1], val_results, lowest_val_loss, early_stop_counter)\n",
    "                else:\n",
    "                    pass\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                print(\"end of training dataset\")\n",
    "        else:\n",
    "            pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
