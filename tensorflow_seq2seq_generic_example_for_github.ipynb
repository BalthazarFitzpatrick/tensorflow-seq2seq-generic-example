{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/python3/3.5.2_3/Frameworks/Python.framework/Versions/3.5/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# math stuff\n",
    "from math import floor\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# TensorFlow\n",
    "import tensorflow as tf\n",
    "\n",
    "# suppress numpy scientific e notation\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.0\n"
     ]
    }
   ],
   "source": [
    "# tensorflow version\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dummy Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dummy data similar to what my database query returns (several features over time)\n",
    "sequence = np.float64(np.arange(20000).reshape((5000, 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Parameters and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of batches in training sequence: 1445\n",
      "# of batches in validation sequence: 945\n",
      "batch size: 3\n",
      "# of training steps: 481\n",
      "# of validation steps: 315\n",
      "run validation op every: 1 training steps\n",
      "new # of batches for training sequence: 315\n",
      "early stop after: 100 validation iterations without improvement\n"
     ]
    }
   ],
   "source": [
    "# sequence variables\n",
    "observation_length = 100 # will cause encoder lstm cells to unroll 'observation_length' times\n",
    "prediction_length = 10 # will cause decoder lstm cells to unroll 'prediction_length' times\n",
    "stride = 2 # window for batch generation will slide by 'stride' time steps\n",
    "combined_length = observation_length + prediction_length\n",
    "total_sequence_length = len(sequence)\n",
    "train_sequence_length = floor(total_sequence_length * 0.6)\n",
    "val_sequence_length = floor(total_sequence_length * 0.4)\n",
    "\n",
    "# compute number of batches to emit\n",
    "num_of_train_seq_batches = floor((train_sequence_length - combined_length) / stride)\n",
    "num_of_val_seq_batches = floor((val_sequence_length - combined_length) / stride)\n",
    "print(\"# of batches in training sequence:\", num_of_train_seq_batches)\n",
    "print(\"# of batches in validation sequence:\", num_of_val_seq_batches)\n",
    "\n",
    "# number of features going into the encoder\n",
    "features_enc_inp = len(sequence[1])\n",
    "\n",
    "# number of features of the target sequence\n",
    "features_dec_inp = 1\n",
    "features_dec_exp_out = features_dec_inp\n",
    "\n",
    "# number of batches used in each iteration\n",
    "batch_size = 3\n",
    "print(\"batch size:\", batch_size)\n",
    "\n",
    "# we're using batch major for now\n",
    "time_major = False\n",
    "\n",
    "# defining layers and number of units for basic lstm cells\n",
    "enc_num_cells = 1 # how many lstm cells are we using\n",
    "enc_num_units = 50 # how many lstm units, or commonly known as hidden dimensions/neurons, shall each lstm cell have\n",
    "dec_num_cells = enc_num_cells\n",
    "dec_num_units = enc_num_units\n",
    "\n",
    "# optimizer variables\n",
    "learning_rate = 0.0003\n",
    "lr_decay = 0.95\n",
    "momentum = 0.5\n",
    "lambda_l2_reg = 0.02\n",
    "\n",
    "# dropout\n",
    "dropout_keep_prob = 0.5\n",
    "\n",
    "# training parameters\n",
    "num_batches_train = floor(num_of_train_seq_batches / batch_size)\n",
    "num_batches_val = floor(num_of_val_seq_batches / batch_size)\n",
    "num_epochs_train = 100\n",
    "early_stop_limit = 100\n",
    "print(\"# of training steps:\", num_batches_train)\n",
    "print(\"# of validation steps:\", num_batches_val)\n",
    "train_to_val_ratio = floor(num_batches_train / num_batches_val)\n",
    "print(\"run validation op every:\", train_to_val_ratio, \"training steps\")\n",
    "num_batches_train = train_to_val_ratio * num_batches_val\n",
    "print(\"new # of batches for training sequence:\", num_batches_train)\n",
    "print(\"early stop after:\", early_stop_limit, \"validation iterations without improvement\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Generator for Training Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define generator function for training data\n",
    "def gen_train():\n",
    "    train_sequence = sequence[0:train_sequence_length]\n",
    "    \n",
    "    # transform and emit data in batches\n",
    "    for i in range(0, num_of_train_seq_batches * stride, stride):\n",
    "        result = np.array(train_sequence[i:i + combined_length])\n",
    "        scaler = MinMaxScaler()\n",
    "        result = scaler.fit_transform(result, y=None)\n",
    "        \n",
    "        # flip array upside down as data is ordered by date desc\n",
    "        result_flipped = np.flipud(result)\n",
    "        \n",
    "        # yield results\n",
    "        yield result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Generator for Validation Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define generator function for validation data\n",
    "def gen_val():\n",
    "    val_sequence = sequence[train_sequence_length:train_sequence_length+val_sequence_length]\n",
    "    \n",
    "    # transform and emit data in batches\n",
    "    for i in range(0, num_of_val_seq_batches * stride, stride):\n",
    "        result = np.array(val_sequence[i:i + combined_length])\n",
    "        scaler = MinMaxScaler()\n",
    "        result = scaler.fit_transform(result, y=None)\n",
    "        \n",
    "        # flip array upside down as data is ordered by date desc\n",
    "        result_flipped = np.flipud(result)\n",
    "        \n",
    "        # yield results\n",
    "        yield result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reset Default TensorFlow Graph and Create Sequence Length Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset default tf graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# length of input and output\n",
    "seq_length_inp = tf.fill([batch_size], observation_length)\n",
    "seq_length_out = tf.fill([batch_size], prediction_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create TensorFlow Training Input and Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create placeholder for training next element\n",
    "train_next_element = tf.placeholder(\n",
    "    tf.float32, \n",
    "    shape=(combined_length, features_enc_inp), \n",
    "    name=\"next_train_element_from_generator\")\n",
    "\n",
    "# create training dataset from generator\n",
    "train_dataset = tf.data.Dataset.from_generator(\n",
    "    gen_train,\n",
    "    (tf.float32),\n",
    "    (tf.TensorShape([combined_length, features_enc_inp]))\n",
    "    )\n",
    "\n",
    "# prefetch 'number of batches' training sequences of data\n",
    "train_prefetched = train_dataset.prefetch(num_of_train_seq_batches)\n",
    "\n",
    "# shuffle training batches\n",
    "train_buffer_size = tf.constant(\n",
    "    num_of_train_seq_batches,\n",
    "    dtype=tf.int64)\n",
    "\n",
    "train_shuffled = train_prefetched.shuffle(\n",
    "    train_buffer_size,\n",
    "    seed=None\n",
    ")\n",
    "\n",
    "# batch training batches together 'batch size' times\n",
    "train_batched = train_shuffled.batch(batch_size)\n",
    "\n",
    "# create initializable training iterator\n",
    "train_iterator = train_batched.make_initializable_iterator()\n",
    "train_next_element = train_iterator.get_next()\n",
    "\n",
    "# create training encoder input slice \n",
    "# from [first batch, first time step in observation sequence, first feature]\n",
    "# to [last batch, last time step in observation sequence, last feature]\n",
    "train_enc_inp = tf.slice(\n",
    "    train_next_element,\n",
    "    [0, 0, 0],\n",
    "    [batch_size, observation_length, features_enc_inp])\n",
    "\n",
    "# create training decoder input slice \n",
    "# from [first batch, first time step in prediction sequence, first feature]\n",
    "# to [last batch, last time step in prediction sequence, first feature]\n",
    "train_dec_inp = tf.slice(\n",
    "    train_next_element,\n",
    "    [0, observation_length, 0],\n",
    "    [batch_size, prediction_length, features_dec_inp])\n",
    "\n",
    "# create training decoder expected output slice \n",
    "# from [first batch, first time step in prediction sequence, first feature]\n",
    "# to [last batch, last time step in prediction sequence, first feature]\n",
    "train_dec_exp_out = tf.slice(\n",
    "    train_next_element,\n",
    "    [0, observation_length, 0],\n",
    "    [batch_size, prediction_length, features_dec_exp_out])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create TensorFlow Validation Input and Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create placeholder for validation next element\n",
    "val_next_element = tf.placeholder(\n",
    "    tf.float32, \n",
    "    shape=(combined_length, features_enc_inp), \n",
    "    name=\"next_val_element_from_generator\")\n",
    "\n",
    "# create validation dataset from generator\n",
    "val_dataset = tf.data.Dataset.from_generator(\n",
    "    gen_val,\n",
    "    (tf.float32),\n",
    "    (tf.TensorShape([combined_length, features_enc_inp]))\n",
    "    )\n",
    "\n",
    "# prefetch 'number of batches' validation sequences of data\n",
    "val_prefetched = val_dataset.prefetch(num_of_val_seq_batches)\n",
    "\n",
    "# shuffle validation batches\n",
    "val_buffer_size = tf.constant(\n",
    "    num_of_val_seq_batches,\n",
    "    dtype=tf.int64)\n",
    "\n",
    "val_shuffled = val_prefetched.shuffle(\n",
    "    val_buffer_size,\n",
    "    seed=None\n",
    ")\n",
    "\n",
    "# batch validation batches together 'batch size' times\n",
    "val_batched = val_shuffled.batch(batch_size)\n",
    "\n",
    "# create initializable validation iterator\n",
    "val_iterator = val_batched.make_initializable_iterator()\n",
    "val_next_element = val_iterator.get_next()\n",
    "\n",
    "# create validation encoder input slice \n",
    "# from [first batch, first time step in observation sequence, first feature]\n",
    "# to [last batch, last time step in observation sequence, last feature]\n",
    "val_enc_inp = tf.slice(\n",
    "    val_next_element,\n",
    "    [0, 0, 0],\n",
    "    [batch_size, observation_length, features_enc_inp])\n",
    "\n",
    "# create validation decoder input slice \n",
    "# from [first batch, first time step in prediction sequence, first feature]\n",
    "# to [last batch, last time step in prediction sequence, first feature]\n",
    "val_dec_inp = tf.slice(\n",
    "    val_next_element,\n",
    "    [0, observation_length, 0],\n",
    "    [batch_size, prediction_length, features_dec_inp])\n",
    "\n",
    "# create validation decoder expected output slice \n",
    "# from [first batch, first time step in prediction sequence, first feature]\n",
    "# to [last batch, last time step in prediction sequence, first feature]\n",
    "val_dec_exp_out = tf.slice(\n",
    "    val_next_element,\n",
    "    [0, observation_length, 0],\n",
    "    [batch_size, prediction_length, features_dec_exp_out])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Encoder and Decoder Cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('Seq2seq', reuse = tf.AUTO_REUSE):\n",
    "\n",
    "    # create encoder cells\n",
    "    enc_cells = []\n",
    "    for i in range(enc_num_cells):\n",
    "        enc_cells.append(tf.nn.rnn_cell.BasicLSTMCell(enc_num_units))\n",
    "    enc_cell = tf.nn.rnn_cell.MultiRNNCell(enc_cells)\n",
    "    enc_cell = tf.nn.rnn_cell.DropoutWrapper(enc_cell, output_keep_prob = dropout_keep_prob)\n",
    "\n",
    "    # create decoder cells\n",
    "    dec_cells = []\n",
    "    for i in range(dec_num_cells):\n",
    "        dec_cells.append(tf.nn.rnn_cell.BasicLSTMCell(dec_num_units))\n",
    "    dec_cell = tf.nn.rnn_cell.MultiRNNCell(dec_cells)\n",
    "    dec_cell = tf.nn.rnn_cell.DropoutWrapper(dec_cell, output_keep_prob = dropout_keep_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('Seq2seq', reuse = tf.AUTO_REUSE): \n",
    "\n",
    "    # define training encoder\n",
    "    train_enc_out, train_enc_state = tf.nn.dynamic_rnn(\n",
    "        enc_cell, \n",
    "        train_enc_inp,\n",
    "        dtype = tf.float32,\n",
    "        sequence_length = seq_length_inp,\n",
    "        time_major = time_major)\n",
    "\n",
    "    # define training training helper\n",
    "    train_training_helper = tf.contrib.seq2seq.TrainingHelper(\n",
    "        train_dec_inp, \n",
    "        seq_length_out, \n",
    "        time_major = time_major)\n",
    "\n",
    "    # define training decoder\n",
    "    train_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "        dec_cell, \n",
    "        train_training_helper, \n",
    "        train_enc_state)\n",
    "\n",
    "    # define training dynamic decoding\n",
    "    train_dec_out, train_dec_state, train_dec_out_seq_length = tf.contrib.seq2seq.dynamic_decode(\n",
    "        train_decoder, \n",
    "        output_time_major = time_major)\n",
    "\n",
    "    # extract training logits from decoder output\n",
    "    train_dec_out_logits = train_dec_out.rnn_output\n",
    "\n",
    "    # training dense layer to reduce output to 'features_dec_exp_out' feature(s)\n",
    "    train_output_dense = tf.layers.dense(\n",
    "        train_dec_out_logits,\n",
    "        features_dec_exp_out)\n",
    "\n",
    "    # training loss function\n",
    "    train_loss = tf.reduce_mean(tf.nn.l2_loss(train_output_dense - train_dec_exp_out))\n",
    "\n",
    "    # l2 regularization using all variables except for bias and batch_resample\n",
    "    l2 = lambda_l2_reg * sum(\n",
    "        tf.nn.l2_loss(tf_var)\n",
    "            for tf_var in tf.trainable_variables()\n",
    "            if not (\"noreg\" in tf_var.name or \"Bias\" in tf_var.name)\n",
    "    )\n",
    "    train_loss += l2\n",
    "\n",
    "    # optimizer type and variables\n",
    "    optimizer = tf.train.RMSPropOptimizer(\n",
    "        learning_rate,\n",
    "        decay = lr_decay,\n",
    "        momentum = momentum,\n",
    "        epsilon = 1e-10)\n",
    "\n",
    "    # train operation\n",
    "    train_op = optimizer.minimize(train_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Validation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('Seq2seq', reuse = tf.AUTO_REUSE):    \n",
    "    \n",
    "    # define validation encoder\n",
    "    val_enc_out, val_enc_state = tf.nn.dynamic_rnn(\n",
    "        enc_cell, \n",
    "        val_enc_inp,\n",
    "        dtype = tf.float32,\n",
    "        sequence_length = seq_length_inp,\n",
    "        time_major = time_major)\n",
    "    \n",
    "    # define validation training helper\n",
    "    val_training_helper = tf.contrib.seq2seq.TrainingHelper(\n",
    "        val_dec_inp, \n",
    "        seq_length_out, \n",
    "        time_major = time_major)\n",
    "    \n",
    "     # define validation decoder\n",
    "    val_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "        dec_cell, \n",
    "        val_training_helper, \n",
    "        val_enc_state)\n",
    "    \n",
    "    # define validation dynamic decoding\n",
    "    val_dec_out, val_dec_state, val_dec_out_seq_length = tf.contrib.seq2seq.dynamic_decode(\n",
    "        val_decoder, \n",
    "        output_time_major = time_major)\n",
    "    \n",
    "    # extract validation logits from decoder output\n",
    "    val_dec_out_logits = val_dec_out.rnn_output\n",
    "    \n",
    "    # validation dense layer to reduce output to 'features_dec_exp_out' feature(s)\n",
    "    val_output_dense = tf.layers.dense(\n",
    "        val_dec_out_logits,\n",
    "        features_dec_exp_out)\n",
    "    \n",
    "    # validation loss function\n",
    "    val_loss = tf.reduce_mean(tf.nn.l2_loss(val_output_dense - val_dec_exp_out))\n",
    "    \n",
    "    # l2 regularization using all variables except for bias and batch_resample\n",
    "    l2 = lambda_l2_reg * sum(\n",
    "        tf.nn.l2_loss(tf_var)\n",
    "            for tf_var in tf.trainable_variables()\n",
    "            if not (\"noreg\" in tf_var.name or \"Bias\" in tf_var.name)\n",
    "    )\n",
    "    val_loss += l2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Trainable Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Seq2seq/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0' shape=(54, 200) dtype=float32_ref>\n",
      "<tf.Variable 'Seq2seq/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0' shape=(200,) dtype=float32_ref>\n",
      "<tf.Variable 'Seq2seq/decoder/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0' shape=(51, 200) dtype=float32_ref>\n",
      "<tf.Variable 'Seq2seq/decoder/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0' shape=(200,) dtype=float32_ref>\n",
      "<tf.Variable 'Seq2seq/dense/kernel:0' shape=(50, 1) dtype=float32_ref>\n",
      "<tf.Variable 'Seq2seq/dense/bias:0' shape=(1,) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "for tf_var in tf.trainable_variables():\n",
    "    print(tf_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Training and Validation Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch batch training_loss validation_loss lowest_val_loss early_stop_counter\n",
      "0 0 10.188868 10.73506 10.73506 0\n",
      "0 1 10.9222975 9.408237 9.408237 0\n",
      "0 2 10.224509 8.082983 8.082983 0\n",
      "0 3 8.0427475 8.664698 8.082983 1\n",
      "0 4 8.139303 7.4073257 7.4073257 0\n",
      "0 5 7.850829 6.485696 6.485696 0\n",
      "0 6 6.912312 6.556961 6.485696 1\n",
      "0 7 6.2842894 5.348409 5.348409 0\n",
      "0 8 5.981259 5.047156 5.047156 0\n",
      "0 9 5.615658 4.9587755 4.9587755 0\n",
      "0 10 4.8780704 4.2950454 4.2950454 0\n",
      "0 11 4.839566 4.481906 4.2950454 1\n",
      "0 12 4.3799343 4.3481326 4.2950454 2\n",
      "0 13 4.7330017 3.6544497 3.6544497 0\n",
      "0 14 3.6890442 4.3452673 3.6544497 1\n",
      "0 15 4.5670166 3.7635574 3.6544497 2\n",
      "0 16 3.3831673 3.5850058 3.5850058 0\n",
      "0 17 3.6612422 3.5139294 3.5139294 0\n",
      "0 18 3.8804169 3.8983788 3.5139294 1\n",
      "0 19 2.7191615 2.8338025 2.8338025 0\n",
      "0 20 3.8989286 2.9804356 2.8338025 1\n",
      "0 21 3.3468904 2.978497 2.8338025 2\n",
      "0 22 2.8058844 3.262672 2.8338025 3\n",
      "0 23 3.1299977 2.7962072 2.7962072 0\n",
      "0 24 2.5803943 2.8808432 2.7962072 1\n",
      "0 25 2.9249802 2.7421834 2.7421834 0\n",
      "0 26 2.5973015 3.0891814 2.7421834 1\n",
      "0 27 2.7817001 3.259284 2.7421834 2\n",
      "0 28 2.7293944 2.6583638 2.6583638 0\n",
      "0 29 2.5908923 2.86994 2.6583638 1\n",
      "0 30 2.721813 2.7497625 2.6583638 2\n",
      "0 31 2.4758558 2.6773338 2.6583638 3\n",
      "0 32 3.0366921 2.9804826 2.6583638 4\n",
      "0 33 2.8418734 2.62369 2.62369 0\n",
      "0 34 2.3605127 2.4999313 2.4999313 0\n",
      "0 35 2.987783 2.5871317 2.4999313 1\n",
      "0 36 2.2283056 2.616743 2.4999313 2\n",
      "0 37 2.4052954 2.4377923 2.4377923 0\n",
      "0 38 2.7773933 2.8295207 2.4377923 1\n",
      "0 39 2.5137568 2.6649203 2.4377923 2\n",
      "0 40 2.3459563 2.5241137 2.4377923 3\n",
      "0 41 2.189073 2.7628117 2.4377923 4\n",
      "0 42 2.815371 2.490091 2.4377923 5\n",
      "0 43 2.5452347 2.425923 2.425923 0\n",
      "0 44 2.3726223 2.6649473 2.425923 1\n",
      "0 45 2.5856867 2.5573614 2.425923 2\n",
      "0 46 2.445934 2.3738222 2.3738222 0\n",
      "0 47 2.53019 2.5414994 2.3738222 1\n",
      "0 48 2.3592582 2.174351 2.174351 0\n",
      "0 49 2.4690576 3.004868 2.174351 1\n",
      "0 50 2.6839724 2.2602925 2.174351 2\n",
      "0 51 2.2399776 2.6475081 2.174351 3\n",
      "0 52 2.701549 2.7612429 2.174351 4\n",
      "0 53 2.4089313 2.528912 2.174351 5\n",
      "0 54 2.6409655 2.4917746 2.174351 6\n",
      "0 55 2.3198752 2.525522 2.174351 7\n",
      "0 56 2.505268 2.4580421 2.174351 8\n",
      "0 57 2.2886481 2.2121215 2.174351 9\n",
      "0 58 2.4498966 2.5282726 2.174351 10\n",
      "0 59 2.4703274 2.305766 2.174351 11\n",
      "0 60 2.1771097 2.8952417 2.174351 12\n",
      "0 61 2.406664 2.1286514 2.1286514 0\n",
      "0 62 2.150525 2.2006915 2.1286514 1\n",
      "0 63 2.4425762 2.4674463 2.1286514 2\n",
      "0 64 2.3573422 2.3784626 2.1286514 3\n",
      "0 65 2.7925315 2.1426246 2.1286514 4\n",
      "0 66 2.1033807 2.3936822 2.1286514 5\n",
      "0 67 2.3902655 2.3885021 2.1286514 6\n",
      "0 68 2.6011343 2.3090713 2.1286514 7\n",
      "0 69 2.5811758 2.2824802 2.1286514 8\n",
      "0 70 2.1459248 2.3008046 2.1286514 9\n",
      "0 71 2.2082374 2.2756915 2.1286514 10\n",
      "0 72 2.3896227 2.2708719 2.1286514 11\n",
      "0 73 2.2315438 2.2998428 2.1286514 12\n",
      "0 74 2.3035204 2.3055344 2.1286514 13\n",
      "0 75 2.4668999 2.2517357 2.1286514 14\n",
      "0 76 2.2946143 2.6195712 2.1286514 15\n",
      "0 77 2.120031 2.326696 2.1286514 16\n",
      "0 78 2.2518387 2.2679923 2.1286514 17\n",
      "0 79 2.338725 2.1602073 2.1286514 18\n",
      "0 80 2.0901237 2.2845922 2.1286514 19\n",
      "0 81 2.298345 2.2040403 2.1286514 20\n",
      "0 82 2.6023233 2.0887516 2.0887516 0\n",
      "0 83 2.620106 2.3739083 2.0887516 1\n",
      "0 84 2.5999453 2.0925174 2.0887516 2\n",
      "0 85 2.1119566 2.1681147 2.0887516 3\n",
      "0 86 2.3115697 2.1828763 2.0887516 4\n",
      "0 87 2.2951546 2.264092 2.0887516 5\n",
      "0 88 2.6431623 2.043442 2.043442 0\n",
      "0 89 2.4433846 2.1006448 2.043442 1\n",
      "0 90 2.1070938 1.9503493 1.9503493 0\n",
      "0 91 2.120693 2.549101 1.9503493 1\n",
      "0 92 2.2396483 2.2844825 1.9503493 2\n",
      "0 93 2.3660648 2.1744313 1.9503493 3\n",
      "0 94 2.2646306 2.2228365 1.9503493 4\n",
      "0 95 2.3417118 2.355442 1.9503493 5\n",
      "0 96 2.2955637 2.3744364 1.9503493 6\n",
      "0 97 2.0934033 2.1067429 1.9503493 7\n",
      "0 98 2.4115303 2.08867 1.9503493 8\n",
      "0 99 2.071216 2.1554549 1.9503493 9\n",
      "0 100 2.2365596 2.334426 1.9503493 10\n",
      "0 101 2.0436757 2.200651 1.9503493 11\n",
      "0 102 2.3411298 2.4212043 1.9503493 12\n",
      "0 103 2.293825 2.182671 1.9503493 13\n",
      "0 104 1.9878716 2.3371687 1.9503493 14\n",
      "0 105 2.1491225 2.408485 1.9503493 15\n",
      "0 106 2.121759 2.1705256 1.9503493 16\n",
      "0 107 1.9919139 2.0795066 1.9503493 17\n",
      "0 108 2.1693082 2.3070939 1.9503493 18\n",
      "0 109 2.139231 2.2852452 1.9503493 19\n",
      "0 110 2.1664712 1.9309878 1.9309878 0\n",
      "0 111 2.0985825 2.2602077 1.9309878 1\n",
      "0 112 2.0375075 2.080731 1.9309878 2\n",
      "0 113 2.068622 2.110227 1.9309878 3\n",
      "0 114 2.1600778 2.1417217 1.9309878 4\n",
      "0 115 2.2691426 1.964482 1.9309878 5\n",
      "0 116 2.3396273 2.1539817 1.9309878 6\n",
      "0 117 2.2175221 1.9748187 1.9309878 7\n",
      "0 118 2.1568916 2.1134574 1.9309878 8\n",
      "0 119 2.1574998 2.2048445 1.9309878 9\n",
      "0 120 2.1602204 2.0498788 1.9309878 10\n",
      "0 121 2.319801 2.1624217 1.9309878 11\n",
      "0 122 2.1224353 2.1719472 1.9309878 12\n",
      "0 123 2.0954328 2.2264147 1.9309878 13\n",
      "0 124 2.105117 1.9463413 1.9309878 14\n",
      "0 125 2.212127 2.163337 1.9309878 15\n",
      "0 126 2.0383892 2.0260532 1.9309878 16\n",
      "0 127 2.0584512 2.0667539 1.9309878 17\n",
      "0 128 2.261269 2.1366596 1.9309878 18\n",
      "0 129 2.1823847 1.8474118 1.8474118 0\n",
      "0 130 1.9627416 1.9658163 1.8474118 1\n",
      "0 131 1.8716027 2.2610257 1.8474118 2\n",
      "0 132 2.200764 1.9376409 1.8474118 3\n",
      "0 133 2.2078743 1.9119837 1.8474118 4\n",
      "0 134 2.0578332 2.0903733 1.8474118 5\n",
      "0 135 2.229149 1.9000983 1.8474118 6\n",
      "0 136 2.009474 1.9905576 1.8474118 7\n",
      "0 137 2.189766 2.069806 1.8474118 8\n",
      "0 138 2.0379472 2.0259924 1.8474118 9\n",
      "0 139 2.0678859 1.8873599 1.8474118 10\n",
      "0 140 1.919164 1.9201224 1.8474118 11\n",
      "0 141 1.9899147 2.0446215 1.8474118 12\n",
      "0 142 1.9814041 1.9679618 1.8474118 13\n",
      "0 143 2.044528 2.0971258 1.8474118 14\n",
      "0 144 1.9616818 1.9128195 1.8474118 15\n",
      "0 145 2.0646813 1.8935378 1.8474118 16\n",
      "0 146 2.1519918 1.9898694 1.8474118 17\n",
      "0 147 1.8408456 2.1480355 1.8474118 18\n",
      "0 148 2.0767946 1.9621096 1.8474118 19\n",
      "0 149 1.9186604 2.2981553 1.8474118 20\n",
      "0 150 1.9656733 1.985693 1.8474118 21\n",
      "0 151 1.8904654 2.08475 1.8474118 22\n",
      "0 152 2.0547917 2.0048642 1.8474118 23\n",
      "0 153 2.0341625 2.0239973 1.8474118 24\n",
      "0 154 2.0006738 2.091154 1.8474118 25\n",
      "0 155 2.0889754 1.9915731 1.8474118 26\n",
      "0 156 2.0115218 1.9630468 1.8474118 27\n",
      "0 157 1.9342782 2.0510242 1.8474118 28\n",
      "0 158 2.0738769 1.9900867 1.8474118 29\n",
      "0 159 1.9577286 2.104486 1.8474118 30\n",
      "0 160 2.10109 1.9419578 1.8474118 31\n",
      "0 161 1.9171067 2.0244265 1.8474118 32\n",
      "0 162 2.0373769 1.9882209 1.8474118 33\n",
      "0 163 2.2685645 2.0322878 1.8474118 34\n",
      "0 164 1.9521959 1.9190598 1.8474118 35\n",
      "0 165 2.10953 2.1693726 1.8474118 36\n",
      "0 166 1.9113268 1.9475156 1.8474118 37\n",
      "0 167 1.9172454 1.8350865 1.8350865 0\n",
      "0 168 1.9469767 1.7716407 1.7716407 0\n",
      "0 169 1.914514 1.9837663 1.7716407 1\n",
      "0 170 1.8955904 1.9062637 1.7716407 2\n",
      "0 171 1.89293 1.8935752 1.7716407 3\n",
      "0 172 1.9622514 1.8889885 1.7716407 4\n",
      "0 173 1.7928497 1.8227518 1.7716407 5\n",
      "0 174 1.923567 1.9114252 1.7716407 6\n",
      "0 175 1.9742017 1.923205 1.7716407 7\n",
      "0 176 2.079245 1.9242972 1.7716407 8\n",
      "0 177 1.9098048 2.2021396 1.7716407 9\n",
      "0 178 1.9379175 1.8596351 1.7716407 10\n",
      "0 179 1.7753847 1.9172561 1.7716407 11\n",
      "0 180 2.0812473 1.8820641 1.7716407 12\n",
      "0 181 1.9533938 1.8860687 1.7716407 13\n",
      "0 182 1.913137 1.9976552 1.7716407 14\n",
      "0 183 1.8922417 1.760856 1.760856 0\n",
      "0 184 1.7631601 1.9510744 1.760856 1\n",
      "0 185 1.7982953 1.9681207 1.760856 2\n",
      "0 186 1.9344689 1.8673817 1.760856 3\n",
      "0 187 1.9541007 1.8823318 1.760856 4\n",
      "0 188 1.8484654 1.9172735 1.760856 5\n",
      "0 189 1.8866708 1.9927148 1.760856 6\n",
      "0 190 1.9652619 1.86813 1.760856 7\n",
      "0 191 1.7526723 1.8857741 1.760856 8\n",
      "0 192 1.8088742 1.8959517 1.760856 9\n",
      "0 193 1.787616 1.9116704 1.760856 10\n",
      "0 194 1.7878864 1.8866154 1.760856 11\n",
      "0 195 1.8324745 1.8523186 1.760856 12\n",
      "0 196 1.8532887 1.9944539 1.760856 13\n",
      "0 197 2.0282044 1.7030963 1.7030963 0\n",
      "0 198 1.7673738 1.7293997 1.7030963 1\n",
      "0 199 1.7495625 1.7809191 1.7030963 2\n",
      "0 200 1.7350681 1.7352402 1.7030963 3\n",
      "0 201 1.8114821 1.7584414 1.7030963 4\n",
      "0 202 1.8019933 1.8379757 1.7030963 5\n",
      "0 203 1.887146 1.7792192 1.7030963 6\n",
      "0 204 1.7144601 1.8943646 1.7030963 7\n",
      "0 205 1.578491 1.8203955 1.7030963 8\n",
      "0 206 1.7352915 1.7489907 1.7030963 9\n",
      "0 207 1.8234677 1.7747208 1.7030963 10\n",
      "0 208 1.6354824 1.7430469 1.7030963 11\n",
      "0 209 1.8469651 1.7768518 1.7030963 12\n",
      "0 210 1.8874097 1.6596584 1.6596584 0\n",
      "0 211 1.857927 1.7697647 1.6596584 1\n",
      "0 212 1.7578368 1.739847 1.6596584 2\n",
      "0 213 1.6254525 1.769023 1.6596584 3\n",
      "0 214 1.6370648 1.7623203 1.6596584 4\n",
      "0 215 1.8680493 1.7052017 1.6596584 5\n",
      "0 216 1.9116275 1.6178334 1.6178334 0\n",
      "0 217 1.7497462 1.7210408 1.6178334 1\n",
      "0 218 1.681261 1.6855483 1.6178334 2\n",
      "0 219 1.6090509 1.6781038 1.6178334 3\n",
      "0 220 1.7303059 1.7126073 1.6178334 4\n",
      "0 221 1.7587633 1.5725737 1.5725737 0\n",
      "0 222 1.5302713 1.6855084 1.5725737 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 223 1.7916329 1.7539783 1.5725737 2\n",
      "0 224 1.6427907 1.6166761 1.5725737 3\n",
      "0 225 1.605714 1.5850879 1.5725737 4\n",
      "0 226 1.6078773 1.61682 1.5725737 5\n",
      "0 227 1.6755089 1.7841877 1.5725737 6\n",
      "0 228 1.6689626 1.5503935 1.5503935 0\n",
      "0 229 1.6048154 1.6307768 1.5503935 1\n",
      "0 230 1.7635236 1.6554654 1.5503935 2\n",
      "0 231 1.5579989 1.7227505 1.5503935 3\n",
      "0 232 1.5725133 1.7547075 1.5503935 4\n",
      "0 233 1.5535668 1.6199689 1.5503935 5\n",
      "0 234 1.5195019 1.6926794 1.5503935 6\n",
      "0 235 1.4929392 1.6372939 1.5503935 7\n",
      "0 236 1.449546 1.4715238 1.4715238 0\n",
      "0 237 1.5836613 1.4832002 1.4715238 1\n",
      "0 238 1.4972197 1.6420125 1.4715238 2\n",
      "0 239 1.64726 1.5229969 1.4715238 3\n",
      "0 240 1.5300462 1.5404892 1.4715238 4\n",
      "0 241 1.5622987 1.4986775 1.4715238 5\n",
      "0 242 1.4351223 1.5605263 1.4715238 6\n",
      "0 243 1.5106213 1.4404945 1.4404945 0\n",
      "0 244 1.5366594 1.4223449 1.4223449 0\n",
      "0 245 1.570168 1.4998376 1.4223449 1\n",
      "0 246 1.4436326 1.4383833 1.4223449 2\n",
      "0 247 1.4239254 1.4326838 1.4223449 3\n",
      "0 248 1.422807 1.504662 1.4223449 4\n",
      "0 249 1.3927897 1.5023025 1.4223449 5\n",
      "0 250 1.507636 1.3414158 1.3414158 0\n",
      "0 251 1.4821496 1.3628651 1.3414158 1\n",
      "0 252 1.6079082 1.4428208 1.3414158 2\n",
      "0 253 1.4395382 1.4593725 1.3414158 3\n",
      "0 254 1.619276 1.5514779 1.3414158 4\n",
      "0 255 1.4411292 1.4425802 1.3414158 5\n",
      "0 256 1.3566613 1.3876522 1.3414158 6\n",
      "0 257 1.3317595 1.3670168 1.3414158 7\n",
      "0 258 1.3948593 1.2790858 1.2790858 0\n",
      "0 259 1.3736769 1.3379409 1.2790858 1\n",
      "0 260 1.2320645 1.3609197 1.2790858 2\n",
      "0 261 1.3591441 1.3182949 1.2790858 3\n",
      "0 262 1.3401169 1.426629 1.2790858 4\n",
      "0 263 1.2643099 1.3266969 1.2790858 5\n",
      "0 264 1.3007089 1.3294622 1.2790858 6\n",
      "0 265 1.3431922 1.2551022 1.2551022 0\n",
      "0 266 1.3011584 1.375013 1.2551022 1\n",
      "0 267 1.3047267 1.2773299 1.2551022 2\n",
      "0 268 1.3188598 1.4649403 1.2551022 3\n",
      "0 269 1.3595111 1.2912967 1.2551022 4\n",
      "0 270 1.3550053 1.2110584 1.2110584 0\n",
      "0 271 1.4190594 1.2617774 1.2110584 1\n",
      "0 272 1.2482575 1.2882594 1.2110584 2\n",
      "0 273 1.2535564 1.3490207 1.2110584 3\n",
      "0 274 1.4033129 1.2922189 1.2110584 4\n",
      "0 275 1.2499273 1.1982566 1.1982566 0\n",
      "0 276 1.2254422 1.3165393 1.1982566 1\n",
      "0 277 1.4047314 1.2099272 1.1982566 2\n",
      "0 278 1.2168896 1.2233444 1.1982566 3\n",
      "0 279 1.1488316 1.2131633 1.1982566 4\n",
      "0 280 1.3382574 1.1228682 1.1228682 0\n",
      "0 281 1.3411956 1.2945184 1.1228682 1\n",
      "0 282 1.1642929 1.1915156 1.1228682 2\n",
      "0 283 1.1618114 1.1861748 1.1228682 3\n",
      "0 284 1.1725568 1.2382997 1.1228682 4\n",
      "0 285 1.2889742 1.2457597 1.1228682 5\n",
      "0 286 1.1840907 1.061559 1.061559 0\n",
      "0 287 1.3227347 1.2250676 1.061559 1\n",
      "0 288 1.2520922 1.2971563 1.061559 2\n",
      "0 289 1.0680999 1.2126353 1.061559 3\n",
      "0 290 1.446168 1.0797596 1.061559 4\n",
      "0 291 1.2505612 1.1047561 1.061559 5\n",
      "0 292 1.1463213 1.0987651 1.061559 6\n",
      "0 293 1.0812604 1.044411 1.044411 0\n",
      "0 294 1.1715384 0.96581364 0.96581364 0\n",
      "0 295 1.0030478 1.1490889 0.96581364 1\n",
      "0 296 1.170592 1.0971051 0.96581364 2\n",
      "0 297 1.1557283 1.0885485 0.96581364 3\n",
      "0 298 1.0702182 1.0029998 0.96581364 4\n",
      "0 299 0.9989749 1.064164 0.96581364 5\n",
      "0 300 1.0728068 0.934267 0.934267 0\n",
      "0 301 1.0786018 1.0546422 0.934267 1\n",
      "0 302 1.010104 1.02916 0.934267 2\n",
      "0 303 0.97602135 1.0073111 0.934267 3\n",
      "0 304 1.0567049 0.9500202 0.934267 4\n",
      "0 305 1.0130086 1.0497553 0.934267 5\n",
      "0 306 1.1608744 1.0481086 0.934267 6\n",
      "0 307 1.0154595 0.9560605 0.934267 7\n",
      "0 308 0.9493154 1.0779213 0.934267 8\n",
      "0 309 0.9962182 0.9890408 0.934267 9\n",
      "0 310 1.0465102 0.95541906 0.934267 10\n",
      "0 311 1.0679637 1.0131334 0.934267 11\n",
      "0 312 1.0341375 1.0675049 0.934267 12\n",
      "0 313 1.0533167 0.9661399 0.934267 13\n",
      "0 314 0.9711207 1.0418634 0.934267 14\n",
      "1 0 0.95213425 0.9478464 0.934267 15\n",
      "1 1 0.96602637 0.95981544 0.934267 16\n",
      "1 2 0.98265254 0.9948318 0.934267 17\n",
      "1 3 1.1257589 0.88849556 0.88849556 0\n",
      "1 4 0.9197387 0.9209827 0.88849556 1\n",
      "1 5 0.94272053 1.013125 0.88849556 2\n",
      "1 6 0.99347454 0.8486733 0.8486733 0\n",
      "1 7 1.0364032 0.9152828 0.8486733 1\n",
      "1 8 0.9469516 0.910979 0.8486733 2\n",
      "1 9 0.91641223 0.98271525 0.8486733 3\n",
      "1 10 1.0546064 0.91205156 0.8486733 4\n",
      "1 11 0.87887967 1.19464 0.8486733 5\n",
      "1 12 0.94477123 0.8625625 0.8486733 6\n",
      "1 13 0.86842656 0.89435595 0.8486733 7\n",
      "1 14 0.92231 0.8957851 0.8486733 8\n",
      "1 15 0.9534514 0.8623826 0.8486733 9\n",
      "1 16 0.8265192 0.8955275 0.8486733 10\n",
      "1 17 0.9072593 0.8420042 0.8420042 0\n",
      "1 18 0.84116966 0.88232064 0.8420042 1\n",
      "1 19 1.0208621 0.8820409 0.8420042 2\n",
      "1 20 0.750886 0.9265178 0.8420042 3\n",
      "1 21 0.96153295 0.7659972 0.7659972 0\n",
      "1 22 0.916083 0.82626784 0.7659972 1\n",
      "1 23 0.8207656 0.84560764 0.7659972 2\n",
      "1 24 0.83738923 0.79757124 0.7659972 3\n",
      "1 25 0.882779 0.8937417 0.7659972 4\n",
      "1 26 0.6936106 0.7963707 0.7659972 5\n",
      "1 27 0.7905385 0.91150784 0.7659972 6\n",
      "1 28 0.7949424 0.8132838 0.7659972 7\n",
      "1 29 0.8080477 0.87355125 0.7659972 8\n",
      "1 30 0.72428584 0.75938594 0.75938594 0\n",
      "1 31 0.79483736 0.86043084 0.75938594 1\n",
      "1 32 0.8221005 0.8115696 0.75938594 2\n",
      "1 33 0.882697 0.7228712 0.7228712 0\n",
      "1 34 0.7309222 0.8248886 0.7228712 1\n",
      "1 35 0.76272595 0.7487308 0.7228712 2\n",
      "1 36 0.7979289 0.8045292 0.7228712 3\n",
      "1 37 0.8413849 0.8466257 0.7228712 4\n",
      "1 38 0.76540756 0.66283303 0.66283303 0\n",
      "1 39 0.75877047 0.8837426 0.66283303 1\n",
      "1 40 0.7585601 0.84660316 0.66283303 2\n",
      "1 41 0.7618836 0.7850441 0.66283303 3\n",
      "1 42 0.86844826 0.71550363 0.66283303 4\n",
      "1 43 0.7939329 0.73386145 0.66283303 5\n",
      "1 44 0.8048578 0.68958724 0.66283303 6\n",
      "1 45 0.90004385 0.8533976 0.66283303 7\n",
      "1 46 0.73239917 0.82579595 0.66283303 8\n",
      "1 47 0.8311088 0.67166775 0.66283303 9\n",
      "1 48 0.6956792 0.69948095 0.66283303 10\n",
      "1 49 0.65815854 0.72021633 0.66283303 11\n",
      "1 50 0.8428441 0.686811 0.66283303 12\n",
      "1 51 0.7225686 0.75126386 0.66283303 13\n",
      "1 52 0.7386848 0.70246136 0.66283303 14\n",
      "1 53 0.6768001 0.75026286 0.66283303 15\n",
      "1 54 0.82470024 0.7444334 0.66283303 16\n",
      "1 55 0.62672585 0.63723695 0.63723695 0\n",
      "1 56 0.8285881 0.73571837 0.63723695 1\n",
      "1 57 0.64675903 0.66668904 0.63723695 2\n",
      "1 58 0.7721933 0.6290572 0.6290572 0\n",
      "1 59 0.61078906 0.63562256 0.6290572 1\n",
      "1 60 0.65445167 0.92312413 0.6290572 2\n",
      "1 61 0.62694246 0.7656956 0.6290572 3\n",
      "1 62 0.6818316 0.65223217 0.6290572 4\n",
      "1 63 0.65328217 0.6059252 0.6059252 0\n",
      "1 64 0.75946134 0.7577568 0.6059252 1\n",
      "1 65 0.60977304 0.62721014 0.6059252 2\n",
      "1 66 0.59626716 0.6276424 0.6059252 3\n",
      "1 67 0.6402743 0.7065712 0.6059252 4\n",
      "1 68 0.741576 0.808263 0.6059252 5\n",
      "1 69 0.67396307 0.595348 0.595348 0\n",
      "1 70 0.77623475 0.6257814 0.595348 1\n",
      "1 71 0.59160924 0.68253684 0.595348 2\n",
      "1 72 0.66285235 0.6584228 0.595348 3\n",
      "1 73 0.56193376 0.6800515 0.595348 4\n",
      "1 74 0.69276655 0.77974653 0.595348 5\n",
      "1 75 0.6055572 0.57755166 0.57755166 0\n",
      "1 76 0.6372054 0.7217486 0.57755166 1\n",
      "1 77 0.6026977 0.5085276 0.5085276 0\n"
     ]
    }
   ],
   "source": [
    "# define session\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# initialize global variables / check for uninitialized variables\n",
    "sess.run([tf.global_variables_initializer(), tf.local_variables_initializer()])\n",
    "\n",
    "# check for unitialized variables \n",
    "# print(sess.run(tf.report_uninitialized_variables())\n",
    "\n",
    "lowest_val_loss = 100.0\n",
    "early_stop_counter = 0\n",
    "\n",
    "# run training and validation\n",
    "print(\"epoch batch training_loss validation_loss lowest_val_loss early_stop_counter\")\n",
    "for epoch in range(num_epochs_train):\n",
    "    sess.run([train_iterator.initializer, val_iterator.initializer])\n",
    "    for batch in range(num_batches_train):\n",
    "        if early_stop_counter <= early_stop_limit:\n",
    "            try:\n",
    "                train_results = sess.run([train_op, train_loss])\n",
    "                if batch % train_to_val_ratio == 0:\n",
    "                    try:\n",
    "                        val_results = sess.run(val_loss)\n",
    "                    except tf.errors.OutOfRangeError:\n",
    "                        print(\"end of validation dataset\")\n",
    "                    if val_results < lowest_val_loss:\n",
    "                        lowest_val_loss = val_results\n",
    "                        early_stop_counter = 0\n",
    "                    else:\n",
    "                        early_stop_counter += 1\n",
    "                    print(epoch, batch, train_results[1], val_results, lowest_val_loss, early_stop_counter)\n",
    "                else:\n",
    "                    pass\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                print(\"end of training dataset\")\n",
    "        else:\n",
    "            pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
